{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evitando UDFs en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los componentes que más me sorprendió en Apache Spark es que permitiera extender el vocabulario de SQL fuera de los límites de DSL, incluso, incrustando funciones de negocio escritas en diferente lenguaje Scala, Java o Python, son admitidos. Antes de evitarlos deberíamos saber ¿que son los UDFs?, ¿cómo ayudan extendiendo las capacidades de SQL?, consideraciones respecto a el rendimiento, por que debería evitar esta maravillosa funcionalidad, ¡bienvenido Column Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido:\n",
    "* [Prerequisitos](#head1)\n",
    "* [Pero Que son los UDF en Apache Spark?](#head2)\n",
    "* [Rendimiento](#head3)\n",
    "* [Bienvenido: Column Functions!](#head4)\n",
    "* [Replacing for Column function](#head1)\n",
    "* [Conclusiones](#head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisitos<a class=\"anchor\" id=\"head1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicamente necesitaremos las siguientes importaciones en nuestro notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, udf}\r\n",
       "import org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pero, Que son los UDF en Apache Spark?<a class=\"anchor\" id=\"head2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined functions o UDF es la forma facil de crear funciones que trabajan registro a registro y que extienden la funcionalidad de SQL, permitiendo construir complejas logicas de negocio y utilizarlas como si fueran funciones nativas de SQL. Este primer ejemplo es el calculo del cuadrado en UDF, escribimos la función <i style=\"color:blue;\">square</i>, registramos ante Spark y estará lista para su uso como función nativa de SQL. Spark serializara las funciones y las enviará a todos los procesos ejecutores en los worker para su ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|UDF(id)|\n",
      "+-------+\n",
      "|      1|\n",
      "|      4|\n",
      "|      9|\n",
      "|     16|\n",
      "|     25|\n",
      "|     36|\n",
      "|     49|\n",
      "|     64|\n",
      "|     81|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "spark.range(1, 10).select(squareUDF(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su versión Inline más compacta con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|UDF:squareUDFInline(id)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "|                     16|\n",
      "|                     25|\n",
      "|                     36|\n",
      "|                     49|\n",
      "|                     64|\n",
      "|                     81|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"squareUDFInline\", (s: Long) => s * s)\n",
    "spark.range(1, 10).createTempView(\"square\")\n",
    "spark.sql(\"SELECT squareUDFInline(id) from square\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones de rendimiento y orden de evaluación<a class=\"anchor\" id=\"head3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una diferencia clave asociada con el lenguaje con el cual se escribio la UDF: si es Java o Scala,  correrán sobre las JVM en las maquinas esclavas, sin embargo si la función fué escrita en Python, Spark iniciara el proceso de Python en el worker, serializara la data a un formato aceptado por Python, ejecutara la función registro a registro en el proceso de Python y finalmente retornará los resultados a la JVM.\n",
    "\n",
    "Estas diferencias en la forma de ejecución traen implicaciones a nivel de rendimiento(<a href=\"https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62\">Spark UDF — Deep insights in performance</a>) ofreciendo evidencia de mejores resultados a las UDFs escritas nativamente en Scala.\n",
    "\n",
    "<img class=\"nh sg s t u he ai hn\" srcset=\"https://miro.medium.com/max/552/1*FFi8Yk6mwSc6AvI-avWcYw.png 276w, https://miro.medium.com/max/1104/1*FFi8Yk6mwSc6AvI-avWcYw.png 552w, https://miro.medium.com/max/1280/1*FFi8Yk6mwSc6AvI-avWcYw.png 640w, https://miro.medium.com/max/1400/1*FFi8Yk6mwSc6AvI-avWcYw.png 700w\" sizes=\"700px\" role=\"presentation\" src=\"https://miro.medium.com/max/1800/1*FFi8Yk6mwSc6AvI-avWcYw.png\" width=\"1200\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que deberia evitar esta maravillosa funcionalidad aún siendo escrita en Scala? Los UDFs no son optimizados por el optimizador de consultas Catalys (<a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Working with UDFs in Apache Spark</a>) y las funciones nativas de SQL a menudo tendran un mejor rendimiento y deberían ser el primer enfoque considerado siempre que se pueda evitar la introducción de un UDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicional se debe tener encuenta que las operaciones de corto circuito\n",
    "<img src=\"src/EvaluationOrderUDF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que no deberian ser nuestra primera elección las UDFs en Apache Spark, como podemos reemplazarlas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenido: Column Functions!<a class=\"anchor\" id=\"head4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su mayoria las Column Functions nos permitaran reescribir código UDF a una función que recibe como parametro una(s) columna(s) y es capaz de retornar una(s) columna(s), se encuentran en el namespace <i style=\"color:blue;\">org.apache.spark.sql.functions</i> (<a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\">Java</a> <a href=\"https://spark.apache.org/docs/2.4.5/api/scala/index.html#org.apache.spark.sql.functions\">Scala</a>). Reescribamos nuestra función <i style=\"color:blue;\">square</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(id * id)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        4|\n",
      "|        9|\n",
      "|       16|\n",
      "|       25|\n",
      "|       36|\n",
      "|       49|\n",
      "|       64|\n",
      "|       81|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareFunction(col:Column):Column = col * col\n",
    "spark.range(1, 10).select(squareFunction(col(\"id\"))).show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
