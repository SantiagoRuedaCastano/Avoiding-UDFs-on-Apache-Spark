{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Functions y porque evitar usar UDFs en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los componentes que más me sorprendió en Apache Spark es que permitiera extender el vocabulario de SQL fuera de los límites de DSL con la ayuda de Column Functions o User-defined functions - UDFS, incluso incrustando funciones de negocio escritas en diferente lenguaje Scala, Java o Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido:\n",
    "* [Prerequisitos](#head1)\n",
    "* [Column Functions y User-defined functions - UDF en Apache Spark](#head2)\n",
    "* [Consideraciones de rendimiento y orden de evaluación UDFs](#head3)\n",
    "* [Column Functions for all!](#head4)\n",
    "* [Reemplazando por for Column function](#head1)\n",
    "* [Conclusiones](#head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisitos<a class=\"anchor\" id=\"head1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicamente necesitaremos las siguientes importaciones en nuestro notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, udf}\r\n",
       "import org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions y User-defined functions - UDF en Apache Spark<a class=\"anchor\" id=\"head2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Column Functions son funciones que reciben como parametro una(s) columna(s) y son capaces de retornar una(s) columna(s), se encuentran en el namespace <i style=\"color:blue;\">org.apache.spark.sql.functions</i> (<a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\">Java</a> <a href=\"https://spark.apache.org/docs/2.4.5/api/scala/index.html#org.apache.spark.sql.functions\">Scala</a>). Al ser funciones nativas, pasan por el optimizador de consultas Catalyst. Escribimos la función <i style=\"color:blue;\">square</i> una Column Functions que calcula el cuadrado de una columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(id * id)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        4|\n",
      "|        9|\n",
      "|       16|\n",
      "|       25|\n",
      "|       36|\n",
      "|       49|\n",
      "|       64|\n",
      "|       81|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(col:Column):Column = col * col\n",
    "spark.range(1, 10).select(square(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined functions o UDF es otra forma de crear funciones que extienden la funcionalidad de SQL, permitiendo construir complejas logicas de negocio y utilizarlas como si fueran funciones nativas de SQL y no relacionadas a tipos de datos asociados a Datasets. Los UDFs requieren ser registradas en Spark y estarán listas para su uso como funciones nativa de SQL. Spark serializará las funciones y las enviará a todos los procesos ejecutores en los worker para su ejecución. Reescribamos nuestra función <i style=\"color:blue;\">square</i> como UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|UDF(id)|\n",
      "+-------+\n",
      "|      1|\n",
      "|      4|\n",
      "|      9|\n",
      "|     16|\n",
      "|     25|\n",
      "|     36|\n",
      "|     49|\n",
      "|     64|\n",
      "|     81|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "spark.range(1, 10).select(squareUDF(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su versión Inline más compacta utilizada con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|UDF:squareUDFInline(id)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "|                     16|\n",
      "|                     25|\n",
      "|                     36|\n",
      "|                     49|\n",
      "|                     64|\n",
      "|                     81|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"squareUDFInline\", (s: Long) => s * s)\n",
    "spark.range(1, 10).createTempView(\"square\")\n",
    "spark.sql(\"SELECT squareUDFInline(id) from square\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones de rendimiento y orden de evaluación UDFs<a class=\"anchor\" id=\"head3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una diferencia clave asociada con el lenguaje con el cual se escribio la UDF: si es Java o Scala,  correrán sobre las JVM en las maquinas esclavas, sin embargo si la función fué escrita en Python, Spark iniciara el proceso de Python en el worker, serializara la data a un formato aceptado por Python, ejecutara la función registro a registro en el proceso de Python y finalmente retornará los resultados a la JVM.\n",
    "\n",
    "Estas diferencias en la forma de ejecución traen implicaciones a nivel de rendimiento(<a href=\"https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62\">Spark UDF — Deep insights in performance</a>) ofreciendo evidencia de mejores resultados las UDFs escritas nativamente en Scala.\n",
    "\n",
    "<img class=\"nh sg s t u he ai hn\" srcset=\"https://miro.medium.com/max/552/1*FFi8Yk6mwSc6AvI-avWcYw.png 276w, https://miro.medium.com/max/1104/1*FFi8Yk6mwSc6AvI-avWcYw.png 552w, https://miro.medium.com/max/1280/1*FFi8Yk6mwSc6AvI-avWcYw.png 640w, https://miro.medium.com/max/1400/1*FFi8Yk6mwSc6AvI-avWcYw.png 700w\" sizes=\"700px\" role=\"presentation\" src=\"https://miro.medium.com/max/1800/1*FFi8Yk6mwSc6AvI-avWcYw.png\" width=\"1200\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que deberia evitar esta maravillosa funcionalidad aún siendo escrita en Scala? Los UDFs no son optimizados por el optimizador de consultas Catalys (<a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Working with UDFs in Apache Spark</a>) y las funciones nativas de SQL a menudo tendran un mejor rendimiento y deberían ser el primer enfoque considerado siempre que se pueda evitar la introducción de un UDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicional se debe tener encuenta que las operaciones de corto circuito\n",
    "<img src=\"src/EvaluationOrderUDF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions for all!<a class=\"anchor\" id=\"head4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su mayoria de veces las Column Functions permitaran reescribir el código de una UDF existente a una versión nativa, los siguientes ejemplos nos podran mostrar su potencial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Functions con condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|       comparative|\n",
      "+---+------------------+\n",
      "|  1|less or equal to 5|\n",
      "|  2|less or equal to 5|\n",
      "|  3|less or equal to 5|\n",
      "|  4|less or equal to 5|\n",
      "|  5|less or equal to 5|\n",
      "|  6|    greater than 5|\n",
      "|  7|    greater than 5|\n",
      "|  8|    greater than 5|\n",
      "|  9|    greater than 5|\n",
      "+---+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(col:Column):Column = {\n",
    "    when(col.leq(lit(5)), lit(\"less or equal to 5\")).otherwise(lit(\"greater than 5\")).as(\"comparative\")\n",
    "}\n",
    "spark.range(1, 10).select(col(\"id\"),square(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-OU1IL3S:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1587550523633)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "24: error: not found: type Column\r",
     "output_type": "error",
     "traceback": [
      "<console>:24: error: not found: type Column\r",
      "       def squareFunction(col:Column):Column = col * col\r",
      "                                      ^",
      "<console>:24: error: not found: type Column\r",
      "       def squareFunction(col:Column):Column = col * col\r",
      "                              ^",
      ""
     ]
    }
   ],
   "source": [
    "Antes de evitarlos deberíamos saber ¿que son los UDFs?, ¿cómo ayudan extendiendo las capacidades de SQL?, consideraciones respecto a el rendimiento, por que debería evitar esta maravillosa funcionalidad, ¡bienvenido Column Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ya que no deberian ser nuestra primera elección las UDFs en Apache Spark, como podemos reemplazarlas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
