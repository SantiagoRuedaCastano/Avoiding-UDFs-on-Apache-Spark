{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Functions Vs User-defined functions - UDFs en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los componentes que más me sorprendió en Apache Spark es que permitiera extender el vocabulario de SQL fuera de los límites de DSL con la ayuda de Column Functions o User-defined functions - UDFs, incluso incrustando funciones de negocio escritas en diferente lenguaje <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Scala</a>, <a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Java</a>, o <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\">Python</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido:\n",
    "* [Prerequisitos](#head1)\n",
    "* [Column Functions en Apache Spark](#head2)\n",
    "* [User-defined functions - UDF en Apache Spark](#head3)\n",
    "* [Consideraciones de rendimiento y orden de evaluación UDFs](#head4)\n",
    "* [Tratamiento Nulls Column Functions y UDFs](#head5)\n",
    "* [Column Functions Vs UDFs](#head6)\n",
    "* [Column Functions for all!](#head7)\n",
    "* [Conclusiones](#head8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisitos<a class=\"anchor\" id=\"head1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Únicamente necesitaremos las siguientes importaciones en nuestro notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-4MHGUNH:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1587599503768)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, udf}\r\n",
       "import org.apache.spark.sql.{Column, Row}\r\n",
       "import org.apache.spark.sql.types.{StructField, StringType, IntegerType, StructType}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.sql.{Column, Row}\n",
    "import org.apache.spark.sql.types.{StructField, StringType, IntegerType, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions en Apache Spark<a class=\"anchor\" id=\"head2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Column Functions son funciones que reciben como parámetro una(s) columna(s) o ninguna, y son capaces de retornar una(s) columna(s), se encuentran en el namespace <i style=\"color:blue;\">org.apache.spark.sql.functions</i> (<a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\">Java</a> <a href=\"https://spark.apache.org/docs/2.4.5/api/scala/index.html#org.apache.spark.sql.functions\">Scala</a>). Al ser funciones nativas, pasan por el optimizador de consultas Catalyst, pero si necesitamos efectuar test a nuestra función demanda un poco de esfuerzo sin la ayuda de librerías como <a href=\"https://github.com/MrPowers/spark-fast-tests\">spark-fast-tests</a> o <a href=\"https://github.com/mockito/mockito-scala\">mockito-scala</a>. Escribamos la función <i style=\"color:blue;\">square</i> una Column Functions que calcula el cuadrado de una columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id * id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        4|\n",
      "|  3|        9|\n",
      "|  4|       16|\n",
      "|  5|       25|\n",
      "|  6|       36|\n",
      "|  7|       49|\n",
      "|  8|       64|\n",
      "|  9|       81|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareFunction(col:Column) = col * col\n",
    "spark.range(10).select(col(\"id\"), squareFunction(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos construir Column Functions sin parámetros de entrada <i style=\"color:blue;\">createNewColum</i> y con condicionales, enteros y literales <i style=\"color:blue;\">comparativeWithValue</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|new column!|       comparative|\n",
      "+---+-----------+------------------+\n",
      "|  0|new column!|less or equal to 5|\n",
      "|  1|new column!|less or equal to 5|\n",
      "|  2|new column!|less or equal to 5|\n",
      "|  3|new column!|less or equal to 5|\n",
      "|  4|new column!|less or equal to 5|\n",
      "|  5|new column!|less or equal to 5|\n",
      "|  6|new column!|    greater than 5|\n",
      "|  7|new column!|    greater than 5|\n",
      "|  8|new column!|    greater than 5|\n",
      "|  9|new column!|    greater than 5|\n",
      "+---+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "createNewColum: ()org.apache.spark.sql.Column\r\n",
       "comparativeWithValue: (col: org.apache.spark.sql.Column, value: Int)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Without input params\n",
    "def createNewColum():Column = lit(\"new column!\") \n",
    "\n",
    "// With integer and literal columns\n",
    "def comparativeWithValue(col:Column, value:Int):Column = {\n",
    "    when(col.leq(lit(value)), lit(s\"less or equal to ${value}\"))\n",
    "        .otherwise(lit(s\"greater than ${value}\")).as(\"comparative\")\n",
    "}\n",
    "\n",
    "spark.range(10).select(col(\"id\"),createNewColum(), comparativeWithValue(col(\"id\"),5)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions - UDF en Apache Spark<a class=\"anchor\" id=\"head3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined functions o UDF es otra forma de crear funciones que extienden la funcionalidad de SQL, permitiendo construir complejas lógicas de negocio y utilizarlas como si fueran funciones nativas de SQL y no relacionadas a tipos de datos asociados a Datasets. Los UDFs requieren ser registradas en Spark y estarán listas para su uso como funciones nativas de SQL. Spark serializará las funciones y las enviará a todos los procesos ejecutores en los worker para su ejecución. Reescribamos nuestra función <i style=\"color:blue;\">square</i> como UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|UDF(id)|\n",
      "+---+-------+\n",
      "|  0|      0|\n",
      "|  1|      1|\n",
      "|  2|      4|\n",
      "|  3|      9|\n",
      "|  4|     16|\n",
      "|  5|     25|\n",
      "|  6|     36|\n",
      "|  7|     49|\n",
      "|  8|     64|\n",
      "|  9|     81|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: Long => Long = <function1>\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val square = (s: Long) => s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "spark.range(10).select(col(\"id\"), squareUDF(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su versión Inline más compacta utilizada con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "| id|UDF:squareUDFInline(id)|\n",
      "+---+-----------------------+\n",
      "|  0|                      0|\n",
      "|  1|                      1|\n",
      "|  2|                      4|\n",
      "|  3|                      9|\n",
      "|  4|                     16|\n",
      "|  5|                     25|\n",
      "|  6|                     36|\n",
      "|  7|                     49|\n",
      "|  8|                     64|\n",
      "|  9|                     81|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"squareUDFInline\", (s: Long) => s * s)\n",
    "spark.range(10).createTempView(\"square\")\n",
    "spark.sql(\"SELECT id, squareUDFInline(id) from square\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar test a una función UDF es bastante sencillo, por ejemplo, un test a nuestra función <i style=\"color:blue;\">square</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: List[Unit] = List((), (), ())\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1, 2, 3).map(x => assert(square(x)== x*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones de rendimiento y orden de evaluación UDFs<a class=\"anchor\" id=\"head4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una diferencia clave asociada con el lenguaje con el cual se escribió la UDF: si es Java o Scala,  correrán sobre las JVM en las maquinas esclavas, sin embargo si la función fue escrita en <a href=\"https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00\">Python</a>, Spark iniciara el proceso de Python en el worker, serializara la data a un formato aceptado por Python, ejecutará la función registro a registro en el proceso de Python y finalmente retornará los resultados a la JVM en la maquina Worker para continuar su procesamiento.\n",
    "\n",
    "Estas diferencias en la forma de ejecución traen implicaciones a nivel de rendimiento(<a href=\"https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62\">Spark UDF — Deep insights in performance</a>) ofreciendo evidencia de mejores resultados las UDFs escritas nativamente en Scala.\n",
    "\n",
    "<img class=\"nh sg s t u he ai hn\" srcset=\"https://miro.medium.com/max/552/1*FFi8Yk6mwSc6AvI-avWcYw.png 276w, https://miro.medium.com/max/1104/1*FFi8Yk6mwSc6AvI-avWcYw.png 552w, https://miro.medium.com/max/1280/1*FFi8Yk6mwSc6AvI-avWcYw.png 640w, https://miro.medium.com/max/1400/1*FFi8Yk6mwSc6AvI-avWcYw.png 700w\" sizes=\"700px\" role=\"presentation\" src=\"https://miro.medium.com/max/1800/1*FFi8Yk6mwSc6AvI-avWcYw.png\" width=\"1200\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué debería evitar esta maravillosa funcionalidad aun siendo escrita en Scala?  Los UDFs no son optimizados por el optimizador de consultas Catalys (<a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Working with UDFs in Apache Spark</a>) y las funciones nativas de SQL a menudo tendrán un mejor rendimiento y deberían ser el primer enfoque considerando siempre que se pueda evitar la introducción de un UDF. Una de las desventajas de UDF es que su invocación cuando se hace a través de spark.sql no puede ser revisada en tiempo de compilación, si la UDF no existe o no se inscribió lanzará una excepción de tipo <i style=\"color:red;\">org.apache.spark.sql.AnalysisException</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.AnalysisException has been launched!!!\n"
     ]
    }
   ],
   "source": [
    "try{\n",
    "    spark.sql(\"SELECT squareNotExists(id) from square\").show\n",
    "} catch{\n",
    "    case x:org.apache.spark.sql.AnalysisException => println(\"org.apache.spark.sql.AnalysisException has been launched!!!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizáramos programación defensiva implica que deberíamos consultar al catálogo de Spark en busca de la existencia de nuestra función UDF y desarrollar alternativas para el manejo de esta situación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----------+---------+-----------+\n",
      "|name           |database|description|className|isTemporary|\n",
      "+---------------+--------+-----------+---------+-----------+\n",
      "|squareUDFInline|null    |null       |null     |true       |\n",
      "+---------------+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listFunctions.filter('name like \"%square%\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro punto clave a tener en cuenta respecto a las UDFs, cuando son usadas en operaciones de filtro a nivel fila o grupo(WHERE o HAVING) es que no tienen garantía de ejecución tales como las operaciones de corto circuito, como lo menciona <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Databricks</a> en <i>Evaluation order and null checking</i>.\n",
    "<img src=\"src/EvaluationOrderUDF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento Nulls Column Functions y UDFs<a class=\"anchor\" id=\"head5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tratamiento de los valores <i style=\"color:green;\">null</i> es diferente en Column Functions y UDFs, miremos con un ejemplo las diferencias en comportamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con Column Functions sobre columnas StringType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|textWithNulls|\n",
      "+-------------+\n",
      "|          AAA|\n",
      "|        ERROR|\n",
      "|          CCC|\n",
      "+-------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [CASE WHEN isnull(value#82) THEN ERROR ELSE upper(value#82) END AS textWithNulls#86]\n",
      "+- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#82]\n",
      "   +- Scan[obj#81]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upperFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "df: org.apache.spark.sql.DataFrame = [textWithNulls: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upperFunction(col:Column):Column = when(col.isNull, lit(\"ERROR\")).otherwise(upper(col)).as(\"textWithNulls\")\n",
    "val df = sc.parallelize(Array(\"aaa\",null,\"ccc\")).toDF(\"id\").select(upperFunction(col(\"id\")))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con UDFs sobre columnas StringType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|  id|textWithNulls|\n",
      "+----+-------------+\n",
      "| aaa|          AAA|\n",
      "|null|        ERROR|\n",
      "| ccc|          CCC|\n",
      "+----+-------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#95 AS id#97, UDF(value#95) AS textWithNulls#99]\n",
      "+- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#95]\n",
      "   +- Scan[obj#94]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upper: (s: String)String\r\n",
       "upperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, textWithNulls: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upper(s: String): String = Option(s).getOrElse(\"error\").toUpperCase\n",
    "val upperUDF = udf(upper(_:String):String)\n",
    "val df = sc.parallelize(Array(\"aaa\",null,\"ccc\")).toDF(\"id\").select(col(\"id\"), upperUDF(col(\"id\")).as(\"textWithNulls\"))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que los resultados son los mismos, se evidencia en el <i style=\"color:gray;\">Physical Plan</i> como la Column Functions fue comprendida <i style=\"color:darkblue;\">CASE WHEN isnull(value#82) THEN err... </i>, mientras que el UDF solo se muestra como una black box: <i style=\"color:darkblue;\">UDF(value#482) AS textWithNulls#486</i>, ahora revisemos el comportamiento con un tipo de columna diferente a StringType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparemos la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataRow: Seq[org.apache.spark.sql.Row] = List([1], [null], [3])\r\n",
       "dataStruct: List[org.apache.spark.sql.types.StructField] = List(StructField(id,IntegerType,true))\r\n",
       "dfdataNull: org.apache.spark.sql.DataFrame = [id: int]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataRow = Seq(Row(1), Row(null), Row(3))\n",
    "val dataStruct = List(StructField(\"id\", IntegerType, nullable = true))\n",
    "val dfdataNull = spark.createDataFrame(spark.sparkContext.parallelize(dataRow), StructType(dataStruct))\n",
    "dfdataNull.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con Column Functions sobre columnas IntegerType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|square|\n",
      "+----+------+\n",
      "|   1|     1|\n",
      "|null|    -1|\n",
      "|   3|     9|\n",
      "+----+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#111, CASE WHEN isnull(id#111) THEN -1 ELSE cast((id#111 * id#111) as string) END AS square#113]\n",
      "+- Scan ExistingRDD[id#111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, square: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareFunction(col:Column):Column = when(col.isNull, lit(\"-1\")).otherwise(col * col).as(\"square\") \n",
    "val df = dfdataNull.select(col(\"id\"), squareFunction(col(\"id\")))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con UDFs sobre columnas IntegerType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|square|\n",
      "+----+------+\n",
      "|   1|     1|\n",
      "|null|  null|\n",
      "|   3|     9|\n",
      "+----+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#111, if (isnull(cast(id#111 as bigint))) null else UDF(cast(id#111 as bigint)) AS square#124L]\n",
      "+- Scan ExistingRDD[id#111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, square: bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = if(s.eq(null)) -1 else s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "val df = dfdataNull.select(col(\"id\"), squareUDF(col(\"id\")).as(\"square\"))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez los resultados son diferentes, y  eso se debe a que Spark cuando invoca una UDF asume una programación defensiva con el tratamiento de los nulos, retornando inmediatamente el valor , sin permitir el manejo de estos valores por parte de las UDFs, a diferencia del comportamiento asumido si corresponde a  la invocación de un Column Functions, si aún esa dudando de esta afirmación, revisemos el <i style=\"color:gray;\">Physical Plan</i> cuando invoca una UDF, pero esta vez en la función <i style=\"color:blue;\">square</i> no escribiremos el manejo de los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [if (isnull(cast(id#111 as bigint))) null else UDF(cast(id#111 as bigint)) AS square#135L]\n",
      "+- Scan ExistingRDD[id#111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "dfdataNull.select(squareUDF(col(\"id\")).as(\"square\")).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es claro que esta línea <i style=\"color:darkblue;\">if (isnull(cast(id#95 as bigint))) null else UDF(cast...</i> no fue adicionada por nuestro código y se debe a un tratamiento interno de Spark para las columnas IntegerType en este caso. Si quisiéramos manejar los valores <i style=\"color:green;\">null</i> con nuestro código y los tipo de columna son numéricos, debemos utilizar Column Functions para este tratamiento y evitar que Spark asuma cierta posición como la vista anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions Vs UDFs<a class=\"anchor\" id=\"head6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tabla comparativa nos resumira los conceptos vistos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "th, td {\n",
    "  padding: 5px;\n",
    "  text-align: center;\n",
    "}\n",
    "th {\n",
    "  text-align: center;\n",
    "}\n",
    "</style>\n",
    "<table>\n",
    "<thead><tr><th></th><th>Column Functions</th><th>UDFs</th></tr></thead><tbody>\n",
    " <tr><td>Sql Functions Native</td><td>Yes</td><td>No</td></tr>\n",
    " <tr><td>Required Spark register</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Compiler check</td><td>Yes</td><td>No(SQL)</td></tr>\n",
    " <tr><td>Return can be different to Colum</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Performance in general</td><td>Fast</td><td>Slow</td></tr>\n",
    " <tr><td>Test</td><td>Medium</td><td>Easy</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las UDFs en Apache Spark no deberian ser nuestra primera elección, bienvenidas Column Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions for all!<a class=\"anchor\" id=\"head7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su mayoría de veces las Column Functions permitarán reescribir el código de una UDF existente a una versión nativa, los siguientes ejemplos nos mostrarán su potencial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accediendo a data externa a Column Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|     fruit|isFruit|\n",
      "+----------+-------+\n",
      "|     apple|    yes|\n",
      "|       car|     no|\n",
      "|     plane|     no|\n",
      "|watermelon|    yes|\n",
      "+----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fruits: List[String] = List(apple, blueberry, watermelon)\r\n",
       "isFruit: org.apache.spark.sql.Column => org.apache.spark.sql.Column = <function1>\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fruits = List(\"apple\",\"blueberry\",\"watermelon\")\n",
    "\n",
    "val isFruit = (col:Column) => when(col.isin(fruits:_*), lit(\"yes\")).otherwise(lit(\"no\")).as(\"isFruit\")\n",
    "\n",
    "sc.parallelize(\n",
    "    Array(\"apple\",\"car\", \"plane\", \"watermelon\")\n",
    ").toDF(\"fruit\").select(col(\"fruit\"), isFruit(col(\"fruit\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando Pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id * id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        4|\n",
      "|  3|        9|\n",
      "|  4|       16|\n",
      "|  5|       25|\n",
      "|  6|       36|\n",
      "|  7|       49|\n",
      "|  8|       64|\n",
      "|  9|       81|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sum: (colA: org.apache.spark.sql.Column, colB: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "diff: (org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function2>\r\n",
       "matchOperation: (operationType: String)(org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column\r\n",
       "genericFunction: (org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function2>\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(colA:Column, colB:Column) = colA + colB \n",
    "val diff = (colA:Column, colB:Column) => colA.minus(colB)\n",
    "\n",
    "def matchOperation(operationType:String):(Column, Column)=>Column = operationType match {\n",
    "  case \"+\" => sum\n",
    "  case \"-\" => diff\n",
    "  case _ => (colA:Column, colB:Column) => colA * colB \n",
    "}\n",
    "\n",
    "val genericFunction = matchOperation(\"other\")\n",
    "spark.range(10).toDF(\"id\").select(col(\"id\"), genericFunction(col(\"id\"), col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando funciones parcialmente aplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id + id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        2|\n",
      "|  2|        4|\n",
      "|  3|        6|\n",
      "|  4|        8|\n",
      "|  5|       10|\n",
      "|  6|       12|\n",
      "|  7|       14|\n",
      "|  8|       16|\n",
      "|  9|       18|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "operate: [A](operation: (A, A) => A, a: A, b: A)A\r\n",
       "deferredOperation: ((org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function1>\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def operate[A](operation:(A, A)=>A, a:A, b:A):A = operation(a,b)\n",
    "val deferredOperation = operate(_:(Column, Column)=>Column, col(\"id\"),col(\"id\"))\n",
    "//Many lines after\n",
    "spark.range(10).toDF(\"id\").select(col(\"id\"), deferredOperation(matchOperation(\"+\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones<a class=\"anchor\" id=\"head8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las UDFs al igual que las Column Functions pueden extender el léxico de SQL, pero siempre las Column Functions deben ser la primera opción para resolver el problema por sus ventajas a nivel de rendimiento y optimizaciones internas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
