{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evitando UDFs en Apache Spark: Column Functions Vs User-defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los componentes que más me sorprendió en Apache Spark es que permitiera extender el vocabulario de SQL fuera de los límites de DSL con la ayuda de Column Functions o User-defined functions - UDFs, incluso incrustando funciones de negocio escritas en diferente lenguaje <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Scala</a>, <a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Java</a>, o <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\">Python</a>. Sin embargo existen ciertos impactos a nivel de rendimiento cuando se usan las UDFs, para esto, vamos a comprender, que son las Column Functions, que son las UDFs, la comparación entre estos componentes y algunas piezas de códigos que ayudarán a evitar usar las UDFs y demostrar el potencial de las Column Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido:\n",
    "* [Prerequisitos](#head1)\n",
    "* [Column Functions en Apache Spark](#head2)\n",
    "* [User-defined functions - UDF en Apache Spark](#head3)\n",
    "* [Consideraciones de rendimiento y orden de evaluación UDFs](#head4)\n",
    "* [Tratamiento Nulls Column Functions y UDFs](#head5)\n",
    "* [Column Functions Vs UDFs](#head6)\n",
    "* [Column Functions para todo!](#head7)\n",
    "* [Conclusiones](#head8)\n",
    "* [Referencias](#head9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisitos<a class=\"anchor\" id=\"head1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Únicamente necesitaremos las siguientes importaciones en nuestro notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-4MHGUNH:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591289054204)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, udf}\r\n",
       "import org.apache.spark.sql.{Column, Row}\r\n",
       "import org.apache.spark.sql.types.{StructField, StringType, IntegerType, StructType}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.sql.{Column, Row}\n",
    "import org.apache.spark.sql.types.{StructField, StringType, IntegerType, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions en Apache Spark<a class=\"anchor\" id=\"head2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Column Functions son funciones que reciben como parámetro una(s) columna(s) o ninguna, y son capaces de retornar una(s) columna(s), se encuentran en el namespace <i style=\"color:blue;\">org.apache.spark.sql.functions</i> (<a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\">Java</a> <a href=\"https://spark.apache.org/docs/2.4.5/api/scala/index.html#org.apache.spark.sql.functions\">Scala</a>). Al ser funciones nativas, pasan por el optimizador de consultas Catalyst, pero si necesitamos efectuar test a nuestra función demanda un poco de esfuerzo sin la ayuda de librerías como <a href=\"https://github.com/MrPowers/spark-fast-tests\">spark-fast-tests</a> o <a href=\"https://github.com/mockito/mockito-scala\">mockito-scala</a>. Escribamos la función <i style=\"color:blue;\">square</i> una Column Functions que calcula el cuadrado de una columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id * id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        4|\n",
      "|  3|        9|\n",
      "|  4|       16|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareFunction(col:Column) = col * col\n",
    "spark.range(5).select(col(\"id\"), squareFunction(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos construir Column Functions sin parámetros de entrada <i style=\"color:blue;\">createNewColum</i> o con condicionales, enteros y literales <i style=\"color:blue;\">comparativeWithValue</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|new column!|       comparative|\n",
      "+---+-----------+------------------+\n",
      "|  0|new column!|less or equal to 2|\n",
      "|  1|new column!|less or equal to 2|\n",
      "|  2|new column!|less or equal to 2|\n",
      "|  3|new column!|    greater than 2|\n",
      "|  4|new column!|    greater than 2|\n",
      "+---+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "createNewColum: ()org.apache.spark.sql.Column\r\n",
       "compareWithValue: (col: org.apache.spark.sql.Column, value: Int)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Without input params\n",
    "def createNewColum():Column = lit(\"new column!\") \n",
    "\n",
    "// With integer and literal columns\n",
    "def compareWithValue(col:Column, value:Int):Column = {\n",
    "    when(col.leq(lit(value)), lit(s\"less or equal to ${value}\"))\n",
    "        .otherwise(lit(s\"greater than ${value}\")).as(\"comparative\")\n",
    "}\n",
    "\n",
    "spark.range(5).select(col(\"id\"),createNewColum(), compareWithValue(col(\"id\"),2)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions - UDF en Apache Spark<a class=\"anchor\" id=\"head3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined functions o UDF es otra forma de crear funciones que extienden la funcionalidad de SQL, permitiendo construir complejas lógicas de negocio y utilizarlas como si fueran funciones nativas de SQL y no relacionadas a tipos de datos asociados a Datasets. Los UDFs requieren ser registradas en Spark y estarán listas para su uso como funciones nativas de SQL. Spark serializará las funciones y las enviará a todos los procesos ejecutores en los worker para su ejecución. Reescribamos nuestra función <i style=\"color:blue;\">square</i> como UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|UDF(id)|\n",
      "+---+-------+\n",
      "|  0|      0|\n",
      "|  1|      1|\n",
      "|  2|      4|\n",
      "|  3|      9|\n",
      "|  4|     16|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: Long => Long = <function1>\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val square = (s: Long) => s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "spark.range(5).select(col(\"id\"), squareUDF(col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su versión Inline más compacta utilizada con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "| id|UDF:squareUDFInline(id)|\n",
      "+---+-----------------------+\n",
      "|  0|                      0|\n",
      "|  1|                      1|\n",
      "|  2|                      4|\n",
      "|  3|                      9|\n",
      "|  4|                     16|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"squareUDFInline\", (s: Long) => s * s)\n",
    "spark.range(5).createTempView(\"square\")\n",
    "spark.sql(\"SELECT id, squareUDFInline(id) from square\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar test a una función UDF es bastante sencillo, por ejemplo, un test a nuestra función <i style=\"color:blue;\">square</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: List[Unit] = List((), (), ())\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1, 2, 3).map(x => assert(square(x)== x*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones de rendimiento y orden de evaluación UDFs<a class=\"anchor\" id=\"head4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una diferencia clave asociada con el lenguaje con el cual se escribió la UDF: si es Java o Scala,  correrán sobre las JVM de los Workers, sin embargo si la función fue escrita en <a href=\"https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00\">Python</a>, Spark iniciara el proceso de Python en el worker, serializara la data a un formato aceptado por Python, ejecutará la función registro a registro en el proceso de Python y finalmente retornará los resultados a la JVM en la maquina Worker para continuar su procesamiento.\n",
    "\n",
    "Estas diferencias en la forma de ejecución traen implicaciones a nivel de rendimiento(<a href=\"https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62\">Spark UDF — Deep insights in performance</a>) ofreciendo evidencia de mejores resultados las UDFs escritas nativamente en Scala.\n",
    "\n",
    "<img class=\"nh sg s t u he ai hn\" srcset=\"https://miro.medium.com/max/552/1*FFi8Yk6mwSc6AvI-avWcYw.png 276w, https://miro.medium.com/max/1104/1*FFi8Yk6mwSc6AvI-avWcYw.png 552w, https://miro.medium.com/max/1280/1*FFi8Yk6mwSc6AvI-avWcYw.png 640w, https://miro.medium.com/max/1400/1*FFi8Yk6mwSc6AvI-avWcYw.png 700w\" sizes=\"700px\" role=\"presentation\" src=\"https://miro.medium.com/max/1800/1*FFi8Yk6mwSc6AvI-avWcYw.png\" width=\"1200\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué debería evitar esta maravillosa funcionalidad aun siendo escrita en Scala?  Los UDFs no son optimizados por el optimizador de consultas Catalys (<a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Working with UDFs in Apache Spark</a>) y las funciones nativas de SQL a menudo tendrán un mejor rendimiento y deberían ser el primer enfoque considerando siempre que se pueda evitar la introducción de un UDF. Otra de las desventajas de UDFs es que su invocación cuando se hace a través de spark.sql no puede ser revisada en tiempo de compilación, si la UDF no existe o no se inscribió, lanzará una excepción de tipo <i style=\"color:red;\">org.apache.spark.sql.AnalysisException</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.AnalysisException has been launched!!!\n"
     ]
    }
   ],
   "source": [
    "try{\n",
    "    spark.sql(\"SELECT squareNotExists(id) from square\").show\n",
    "} catch{\n",
    "    case x:org.apache.spark.sql.AnalysisException => println(\"org.apache.spark.sql.AnalysisException has been launched!!!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizáramos programación defensiva implica que deberíamos consultar al catálogo de Apache Spark en busca de la existencia de nuestra función UDF y desarrollar alternativas para el manejo de esta situación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----------+---------+-----------+\n",
      "|name           |database|description|className|isTemporary|\n",
      "+---------------+--------+-----------+---------+-----------+\n",
      "|squareUDFInline|null    |null       |null     |true       |\n",
      "+---------------+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listFunctions.filter('name like \"%square%\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro punto clave a tener en cuenta respecto a las UDFs, cuando son usadas en operaciones de filtro a nivel fila o grupo(WHERE o HAVING) es que no tienen garantía de ejecución tales como las operaciones de corto circuito, como lo menciona Databricks en <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Evaluation order and null checking</a>.\n",
    "<img src=\"src/EvaluationOrderUDF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento Nulls Column Functions y UDFs<a class=\"anchor\" id=\"head5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tratamiento de los valores <i style=\"color:green;\">null</i> puede ser diferente en Column Functions y UDFs bajo ciertas condiciones, miremos con un ejemplo las diferencias en comportamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con Column Functions sobre columnas StringType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|  id|textWithNulls|\n",
      "+----+-------------+\n",
      "| aaa|          AAA|\n",
      "|null|        ERROR|\n",
      "| ccc|          CCC|\n",
      "+----+-------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#82 AS id#84, CASE WHEN isnull(value#82) THEN ERROR ELSE upper(value#82) END AS textWithNulls#86]\n",
      "+- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#82]\n",
      "   +- Scan[obj#81]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upperFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, textWithNulls: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upperFunction(col:Column):Column = when(col.isNull, lit(\"ERROR\")).otherwise(upper(col)).as(\"textWithNulls\")\n",
    "val df = sc.parallelize(Array(\"aaa\",null,\"ccc\")).toDF(\"id\").select(col(\"id\"),upperFunction(col(\"id\")))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con UDFs sobre columnas StringType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|  id|textWithNulls|\n",
      "+----+-------------+\n",
      "| aaa|          AAA|\n",
      "|null|        ERROR|\n",
      "| ccc|          CCC|\n",
      "+----+-------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#99 AS id#101, UDF(value#99) AS textWithNulls#103]\n",
      "+- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#99]\n",
      "   +- Scan[obj#98]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upper: (s: String)String\r\n",
       "upperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, textWithNulls: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upper(s: String): String = Option(s).getOrElse(\"error\").toUpperCase\n",
    "val upperUDF = udf(upper(_:String):String)\n",
    "val df = sc.parallelize(Array(\"aaa\",null,\"ccc\")).toDF(\"id\").select(col(\"id\"), upperUDF(col(\"id\")).as(\"textWithNulls\"))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analizando los resultados: columna StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/StringType_SideToSide.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que los resultados son los mismos, se evidencia en el <i style=\"color:gray;\">Physical Plan</i> como la Column Functions(izquierda) fue comprendida <i style=\"color:darkblue;\">CASE WHEN isnull(value#82) THEN ERR... </i>, mientras que el UDF(derecha) solo se muestra como una black box: <i style=\"color:darkblue;\">UDF(value#99) AS textWithNulls#103</i>, ahora revisemos el comportamiento con un tipo de columna diferente a StringType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparemos la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataRow: Seq[org.apache.spark.sql.Row] = List([1], [null], [3])\r\n",
       "dataStruct: List[org.apache.spark.sql.types.StructField] = List(StructField(id,IntegerType,true))\r\n",
       "dfdataNull: org.apache.spark.sql.DataFrame = [id: int]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataRow = Seq(Row(1), Row(null), Row(3))\n",
    "val dataStruct = List(StructField(\"id\", IntegerType, nullable = true))\n",
    "val dfdataNull = spark.createDataFrame(spark.sparkContext.parallelize(dataRow), StructType(dataStruct))\n",
    "dfdataNull.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con Column Functions sobre columnas IntegerType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|square|\n",
      "+----+------+\n",
      "|   1|     1|\n",
      "|null|    -1|\n",
      "|   3|     9|\n",
      "+----+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#115, CASE WHEN isnull(id#115) THEN -1 ELSE cast((id#115 * id#115) as string) END AS square#117]\n",
      "+- Scan ExistingRDD[id#115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "squareFunction: (col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, square: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squareFunction(col:Column):Column = when(col.isNull, lit(\"-1\")).otherwise(col * col).as(\"square\") \n",
    "val df = dfdataNull.select(col(\"id\"), squareFunction(col(\"id\")))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nulls con UDFs sobre columnas IntegerType y el Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|square|\n",
      "+----+------+\n",
      "|   1|     1|\n",
      "|null|  null|\n",
      "|   3|     9|\n",
      "+----+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#115, if (isnull(cast(id#115 as bigint))) null else UDF(cast(id#115 as bigint)) AS square#128L]\n",
      "+- Scan ExistingRDD[id#115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, square: bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = if(s.eq(null)) -1 else s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "val df = dfdataNull.select(col(\"id\"), squareUDF(col(\"id\")).as(\"square\"))\n",
    "df.show\n",
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analizando los resultados: columna IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/IntegerType_SideToSide.png\" width=\"856\" height=\"215\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez los resultados son diferentes, y  eso se debe a que Spark cuando invoca una UDF(derecha) asume una programación defensiva con el tratamiento de los nulos, retornando inmediatamente el valor <i>null</i>, sin permitir el manejo de estos valores por parte de las UDFs, a diferencia del comportamiento asumido si corresponde a  la invocación de un Column Functions(izquierda), si aún duda de esta afirmación, revisemos el <i style=\"color:gray;\">Physical Plan</i> cuando invoca una UDF, pero esta vez en la función <i style=\"color:blue;\">square</i> no escribiremos el manejo de los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [if (isnull(cast(id#115 as bigint))) null else UDF(cast(id#115 as bigint)) AS square#139L]\n",
      "+- Scan ExistingRDD[id#115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "square: (s: Long)Long\r\n",
       "squareUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(s: Long) = s * s\n",
    "val squareUDF = udf(square(_:Long):Long)\n",
    "dfdataNull.select(squareUDF(col(\"id\")).as(\"square\")).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es claro que esta línea <i style=\"color:darkblue;\">if (isnull(cast(id#95 as bigint))) null else ...</i> no fue adicionada por nuestro código y se debe a un tratamiento interno de Apache Spark para las columnas IntegerType en este caso. Si quisiéramos manejar los valores <i style=\"color:green;\">null</i> con nuestro código y los tipo de columna son: <i style=color:darkblue>BooleanType, ByteType, ShortType, IntegerType, FloatType, DoubleType, StringType, DecimalType</i>, debemos utilizar Column Functions para este tratamiento y evitar que Apache Spark asuma la posición como vimos anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions Vs UDFs<a class=\"anchor\" id=\"head6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tabla comparativa nos resumira los conceptos vistos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "th, td {\n",
    "  padding: 5px;\n",
    "  text-align: center;\n",
    "}\n",
    "th {\n",
    "  text-align: center;\n",
    "}\n",
    "</style>\n",
    "<table>\n",
    "<thead><tr><th></th><th>Column Functions</th><th>UDFs</th></tr></thead><tbody>\n",
    " <tr><td>Sql Functions Native</td><td>Yes</td><td>No</td></tr>\n",
    " <tr><td>Required Spark register</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Compiler check</td><td>Yes</td><td>No(SQL)</td></tr>\n",
    " <tr><td>Return can be different to Colum</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Can be optimized by Catalyst</td><td>Yes</td><td>No</td></tr>\n",
    " <tr><td>Performance in general</td><td>Fast</td><td>Slow</td></tr>\n",
    " <tr><td>Test</td><td>Medium</td><td>Easy</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las UDFs en Apache Spark no deberian ser nuestra primera elección, bienvenidas Column Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Functions para todo!<a class=\"anchor\" id=\"head7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su mayoría de veces las Column Functions permitarán reescribir el código de una UDF existente a una versión nativa, los siguientes ejemplos nos mostrarán su potencial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accediendo a data externa a Column Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las column functions permiten acceder a data externa a la declaración, facilitando comparaciones con datos  no alojados dentro de la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|     fruit|isFruit|\n",
      "+----------+-------+\n",
      "|     apple|    yes|\n",
      "|       car|     no|\n",
      "|     plane|     no|\n",
      "|watermelon|    yes|\n",
      "+----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fruits: List[String] = List(apple, blueberry, watermelon)\r\n",
       "isFruit: org.apache.spark.sql.Column => org.apache.spark.sql.Column = <function1>\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fruits = List(\"apple\",\"blueberry\",\"watermelon\")\n",
    "\n",
    "val isFruit = (col:Column) => when(col.isin(fruits:_*), lit(\"yes\")).otherwise(lit(\"no\")).as(\"isFruit\")\n",
    "\n",
    "sc.parallelize(\n",
    "    Array(\"apple\",\"car\", \"plane\", \"watermelon\")\n",
    ").toDF(\"fruit\").select(col(\"fruit\"), isFruit(col(\"fruit\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando Pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, usamos el Pattern Matching para seleccionar el tipo de Column Function a aplicar sobre las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id * id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        4|\n",
      "|  3|        9|\n",
      "|  4|       16|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sum: (colA: org.apache.spark.sql.Column, colB: org.apache.spark.sql.Column)org.apache.spark.sql.Column\r\n",
       "diff: (org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function2>\r\n",
       "matchOperation: (operationType: String)(org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column\r\n",
       "genericFunction: (org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function2>\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(colA:Column, colB:Column) = colA + colB \n",
    "val diff = (colA:Column, colB:Column) => colA.minus(colB)\n",
    "\n",
    "def matchOperation(operationType:String):(Column, Column)=>Column = operationType match {\n",
    "  case \"+\" => sum\n",
    "  case \"-\" => diff\n",
    "  case _ => (colA:Column, colB:Column) => colA * colB \n",
    "}\n",
    "\n",
    "val genericFunction = matchOperation(\"other\")\n",
    "spark.range(5).toDF(\"id\").select(col(\"id\"), genericFunction(col(\"id\"), col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando funciones parcialmente aplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos suministrar únicamente las columnas involucradas inicialmente, y posterior definir qué operación realizar sobre ellas gracias a las funciones parcialmente aplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|(id + id)|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        2|\n",
      "|  2|        4|\n",
      "|  3|        6|\n",
      "|  4|        8|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "operate: [A](operation: (A, A) => A, a: A, b: A)A\r\n",
       "deferredOperation: ((org.apache.spark.sql.Column, org.apache.spark.sql.Column) => org.apache.spark.sql.Column) => org.apache.spark.sql.Column = <function1>\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def operate[A](operation:(A, A)=>A, a:A, b:A):A = operation(a,b)\n",
    "val deferredOperation = operate(_:(Column, Column)=>Column, col(\"id\"),col(\"id\"))\n",
    "//Many lines after\n",
    "spark.range(5).toDF(\"id\").select(col(\"id\"), deferredOperation(matchOperation(\"+\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recibiendo multiples columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede crear Column Functions que reciban <i>n</i> argumentos de entrada y retornen una única columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|concat(test, id)|\n",
      "+----------------+\n",
      "|           test0|\n",
      "|           test1|\n",
      "|           test2|\n",
      "|           test3|\n",
      "|           test4|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "concatColumns: (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatColumns(cols: Column*):Column = concat(cols:_*)\n",
    "spark.range(5).toDF(\"id\").select(concatColumns(lit(\"test\"), col(\"id\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retornando multiples columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera es factible crear Column Functions que retornen una <i>List</i> conteniendo múltiples columnas y anexándolas a las columnas ya existentes en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "|  0|  a|  b|  c|\n",
      "|  1|  a|  b|  c|\n",
      "|  2|  a|  b|  c|\n",
      "|  3|  a|  b|  c|\n",
      "|  4|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "getMultipleColumns: ()List[org.apache.spark.sql.Column]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMultipleColumns():List[Column] = {\n",
    "    val lstColumns = List(lit(\"a\"), lit(\"b\"), lit(\"c\"))\n",
    "    lstColumns\n",
    "}\n",
    "spark.range(5).toDF(\"id\").select(col(\"id\") +: getMultipleColumns:_*).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplicando formato a una columna de tipo DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede externalizar el formato deseado de la columna, siempre y cuando se ha un formato valido para la clase <a href=\"https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html\">DateTimeFormatter</a> en Java, que son las especificadas por el <a href=\"https://www.ietf.org/rfc/rfc3339.txt\">RFC3339</a> estandar ISO8601.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------------------------+------+----------------+\n",
      "|id |current_date|yyyy-MM-dd'T'HH:mm:ss.SSSSSS|MMM-yy|yyyy MMMM dd E  |\n",
      "+---+------------+----------------------------+------+----------------+\n",
      "|0  |2020-06-04  |2020-06-04T09:44:30.000786  |Jun-20|2020 June 04 Thu|\n",
      "|1  |2020-06-04  |2020-06-04T09:44:30.000786  |Jun-20|2020 June 04 Thu|\n",
      "|2  |2020-06-04  |2020-06-04T09:44:30.000786  |Jun-20|2020 June 04 Thu|\n",
      "|3  |2020-06-04  |2020-06-04T09:44:30.000786  |Jun-20|2020 June 04 Thu|\n",
      "|4  |2020-06-04  |2020-06-04T09:44:30.000786  |Jun-20|2020 June 04 Thu|\n",
      "+---+------------+----------------------------+------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "applyFormat: (org.apache.spark.sql.Column, String) => org.apache.spark.sql.Column = <function2>\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val applyFormat = (col:Column, format:String) => date_format(col,format).as(format)\n",
    "spark.range(5).toDF(\"id\").select(col(\"id\"),\n",
    "    current_date().as(\"current_date\"),\n",
    "    applyFormat(current_timestamp(),\"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"),\n",
    "    applyFormat(current_timestamp(),\"MMM-yy\"),\n",
    "    applyFormat(current_timestamp(),\"yyyy MMMM dd E\")\n",
    "  ).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones<a class=\"anchor\" id=\"head8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las UDFs al igual que las Column Functions pueden extender el léxico de SQL, pero siempre las Column Functions deben ser la primera opción para resolver el problema por sus ventajas a nivel de rendimiento y optimizaciones internas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias: <a class=\"anchor\" id=\"head9\"></a>\n",
    "* <a href=\"https://blog.cloudera.com/working-with-udfs-in-apache-spark/\">Working with UDFs in Apache Spark</a>\n",
    "* <a href=\"https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00\">pyspark UDF-deep-dive</a>\n",
    "* <a href=\"https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62\">Spark UDF — Deep insights in performance</a>\n",
    "* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Databricks - UDF Scala</a>\n",
    "* <a href=\"https://pages.databricks.com/definitive-guide-spark.html\">Spark: The Definitive Guide</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
